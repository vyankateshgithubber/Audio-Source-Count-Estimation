{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MajorProject1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vyankateshgithubber/Audio-Source-Count-Estimation/blob/main/MiniDataSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP7F1w_mmymM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096c59d6-248d-4e78-d71c-b50bcb5c570a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4rImvrx7kwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fada6d-1b95-434b-8cc5-ec75612617a4"
      },
      "source": [
        "!pip install wavefile\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy.core.fromnumeric import size\n",
        "from scipy.io import loadmat\n",
        "from scipy.io.wavfile import write\n",
        "import wavefile\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "def plot(h1,s1,convolved_speech_data1,h2,s2,convolved_speech_data2,mixed_signal) :\n",
        "  plt.subplot(3,1,1)\n",
        "  plt.subplots_adjust(hspace=1,wspace=5)\n",
        "  plt.title('Room Filter-1')\n",
        "  plt.plot(np.arange(0,80000),h1[:80000])\n",
        "  plt.subplot(3,1,2)\n",
        "  plt.title('Raw Source-1')\n",
        "  plt.plot(s1)\n",
        "  plt.subplot(3,1,3)\n",
        "  plt.title('Convolved Source-1 with Filter-1')\n",
        "  plt.plot(convolved_speech_data1)\n",
        "  plt.savefig('/content/drive/MyDrive/MajorProject/signal1.png')\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "  plt.subplot(3,1,1)\n",
        "  plt.subplots_adjust(hspace=1,wspace=5)\n",
        "  plt.title('Room Filter-1')\n",
        "  plt.plot(np.arange(0,80000),h2[:80000])\n",
        "  plt.subplot(3,1,2)\n",
        "  plt.title('Raw Source-1')\n",
        "  plt.plot(s2)\n",
        "  plt.subplot(3,1,3)\n",
        "  plt.title('Convolved Source-1 with Filter-1')\n",
        "  plt.plot(convolved_speech_data2)\n",
        "  plt.savefig('/content/drive/MyDrive/MajorProject/signal2.png')\n",
        "  plt.close()\n",
        "\n",
        "  \n",
        "  plt.plot(mixed_signal)\n",
        "  plt.title('Mixed Source-1 and Source-2')\n",
        "  plt.savefig('/content/drive/MyDrive/MajorProject/mixedsignal.png')\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "mat1 = loadmat('/content/drive/MyDrive/MajorProject/45.mat')\n",
        "h1 = mat1['impulse_response'][:,0]\n",
        "fs1, s1 = wavefile.load('/content/drive/MyDrive/MajorProject/s2_male.wav')\n",
        "s1=np.reshape(s1,s1.size)\n",
        "convolved_speech_data1 = np.convolve(s1[:80000],h1[:5000])\n",
        "\n",
        "fs3, s3 = wavefile.load('/content/drive/MyDrive/MajorProject/s3_female.wav')\n",
        "s3=np.reshape(s3,s3.size)\n",
        "convolved_speech_data3 = np.convolve(s3[:80000],h1[:5000])\n",
        "\n",
        "\n",
        "mat2 = loadmat('/content/drive/MyDrive/MajorProject/135.mat')\n",
        "fs2, s2 = wavefile.load('/content/drive/MyDrive/MajorProject/s1_female.wav')\n",
        "s2=np.reshape(s2,s2.size)\n",
        "h2 = mat2['impulse_response'][:,0]\n",
        "convolved_speech_data2 = np.convolve(s2[:80000],h2[:5000])\n",
        "\n",
        "fs4, s4 = wavefile.load('/content/drive/MyDrive/MajorProject/s4_male.wav')\n",
        "s4=np.reshape(s4,s4.size)\n",
        "convolved_speech_data4 = np.convolve(s4[:80000],h2[:5000])\n",
        "\n",
        "\n",
        "#plotting and storing a sample signal\n",
        "mixed_signal = convolved_speech_data1+convolved_speech_data2\n",
        "print(mixed_signal.size)\n",
        "#plot(h1,s1,convolved_speech_data1,h2,s2,convolved_speech_data2,mixed_signal)   \n",
        "\n",
        "\n",
        "\n",
        "#creating the dataset that has two sources\n",
        "for k1 in np.arange(0.2,1.2,0.2):\n",
        "  for k2 in np.arange(0.2,1.2,0.2):\n",
        "    mixed_signal = (k1*convolved_speech_data1) + (k2*convolved_speech_data2)\n",
        "    write('/content/drive/MyDrive/MajorProject/AudioDataset/'+'2_'+str(round(k1,3))+'_'+str(round(k2,3))+'.wav',fs2,mixed_signal.astype(np.float32))\n",
        "\n",
        "#creating the dataset that has either s1_female and s2_male\n",
        "flag= True\n",
        "for k1 in np.arange(0.1,1.1,0.1):\n",
        "  if flag==True:\n",
        "    convolved_speech = k1*convolved_speech_data1\n",
        "    write('/content/drive/MyDrive/MajorProject/AudioDataset/'+'1_'+str(round(k1,3))+'_'+str(round(0,3))+'.wav',fs1,convolved_speech.astype(np.float32))\n",
        "  else:\n",
        "    convolved_speech = k1*convolved_speech_data2\n",
        "    write('/content/drive/MyDrive/MajorProject/AudioDataset/'+'1_'+str(round(k1,3))+'_'+str(round(0,3))+'.wav',fs1,convolved_speech.astype(np.float32))\n",
        "\n",
        "  flag = not flag\n",
        "\n",
        "#creating the dataset that has either s3_female and s4_male\n",
        "flag= True\n",
        "for k1 in np.arange(0.1,1.1,0.1):\n",
        "  if flag==True:\n",
        "    convolved_speech= k1*convolved_speech_data3\n",
        "    write('/content/drive/MyDrive/MajorProject/AudioDataset/'+'1_'+str(round(0,3))+'_'+str(round(k1,3))+'.wav',fs1,convolved_speech.astype(np.float32))\n",
        "  else:\n",
        "    convolved_speech= k1*convolved_speech_data4\n",
        "    write('/content/drive/MyDrive/MajorProject/AudioDataset/'+'1_'+str(round(0,3))+'_'+str(round(k1,3))+'.wav',fs1,convolved_speech.astype(np.float32))\n",
        "\n",
        "  flag = not flag\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wavefile in /usr/local/lib/python3.7/dist-packages (1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from wavefile) (1.19.5)\n",
            "84999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snmNrJ_WoX9a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}